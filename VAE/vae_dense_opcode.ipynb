{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected (Dense) Variational Autoencoder \n",
    "\n",
    "Authored by David Luong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow/Keras: 2.7.0\n",
      "numpy: 1.21.4\n",
      "matplotlib: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow / Keras\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Model # for creating assembling a Neural Network model\n",
    "from keras import Input # for instantiating a keras tensor and specifying input dimensions\n",
    "from keras.layers import Dense, Lambda # adding layers to the Neural Network model\n",
    "from tensorflow.keras.utils import plot_model # for plotting model diagram\n",
    "from keras import backend as K # for access to Keras backend for reparameterization and creating custom loss function\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np # for data manipulation\n",
    "print('numpy: %s' % np.__version__) # print version\n",
    "\n",
    "# Visualization\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt # for plotting model loss\n",
    "print('matplotlib: %s' % matplotlib.__version__) # print version\n",
    "#import graphviz # for showing model diagram\n",
    "#print('graphviz: %s' % graphviz.__version__) # print version\n",
    "#import plotly\n",
    "#import plotly.express as px # for data visualization\n",
    "#print('plotly: %s' % plotly.__version__) # print version\n",
    "\n",
    "#Import svm model\n",
    "from sklearn import svm\n",
    "#Import random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "#Import knn model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Final evaluation of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score, precision_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Other utilities\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Assign main directory to a variable\n",
    "main_dir=os.path.dirname(sys.path[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Run Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# malware family\n",
    "real_malware = 'Mixed' # OnLineGames, Renos, VBInject, WinWebSec, Zbot, Mixed\n",
    "\n",
    "# data processing\n",
    "use_harshit = 0 # 0 = use our samples, other to use Harshit's fake samples\n",
    "unique_opcodes = 1 # 0 = visualize_explore/opcodes, 1 = fake_tests/opdicts\n",
    "\n",
    "# VAE generator parameters\n",
    "max_sequence_length = 600 # number of opcodes at the input layer (same as maximum length sequence)\n",
    "latent_dim = 2 # latent space dimension\n",
    "num_epochs = 30\n",
    "num_batch_size = 16\n",
    "use_trained_weights = 1 # 0 = train VAE, 1 = use saved train weights\n",
    "\n",
    "# Classifier parameters\n",
    "embedding_vector_length = 32\n",
    "use_deep_classifiers = 1\n",
    "run_classifiers = 1\n",
    "use_kfold_deep_classifiers = 0\n",
    "target_names = ['Authentic', 'Synthetic']\n",
    "classify_test_size = 0.2\n",
    "\n",
    "# save directory\n",
    "fp = './results/vae_dense/'\n",
    "write_opcode_files = 0\n",
    "write_opcode_images = 0\n",
    "\n",
    "# derived parameters\n",
    "if real_malware == 'WinWebSec':\n",
    "    fake_malware = 'wws'\n",
    "    savedir_malware = 'wws'\n",
    "    top_opcodes = 22\n",
    "elif real_malware == 'OnLineGames':\n",
    "    fake_malware = 'olgames'\n",
    "    savedir_malware = 'olgames'\n",
    "    top_opcodes = 22\n",
    "elif real_malware == 'Renos':\n",
    "    fake_malware = 'renos'\n",
    "    savedir_malware = 'renos'\n",
    "    top_opcodes = 22\n",
    "elif real_malware == 'VBInject':\n",
    "    fake_malware = 'vbinject'\n",
    "    savedir_malware = 'vbinject'\n",
    "    top_opcodes = 25\n",
    "elif real_malware == 'Zbot':\n",
    "    fake_malware = 'zbot'\n",
    "    savedir_malware = 'zbot'\n",
    "    top_opcodes = 20\n",
    "elif real_malware == 'Mixed':\n",
    "    fake_malware = 'mixed'\n",
    "    savedir_malware = 'mixed'\n",
    "    top_opcodes = 41\n",
    "else:\n",
    "    print('!!!Malware not found!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get unique values\n",
    "def unique(list1):\n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list\n",
    "\n",
    "# Python code to count the number of occurrences\n",
    "def countX(lst, x):\n",
    "    count = 0\n",
    "    for ele in lst:\n",
    "        if (ele == x):\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "# opcodes are indexed by overall frequency\n",
    "def get_opcode_freq(opcode_frequency):\n",
    "    rank = 0\n",
    "    prev_count = -1\n",
    "    for item in sorted(opcode_frequency,reverse=True):\n",
    "        # increment rank if current opcode has different frequency than previous opcode\n",
    "        if prev_count != item[0]:\n",
    "            rank+=1\n",
    "        # assign frequency rank\n",
    "        opcode_frequency[item[1]] = rank\n",
    "        # save previous frequency\n",
    "        prev_count = item[0]\n",
    "    return opcode_frequency\n",
    "\n",
    "# opcodes are indexed by unique opcode position (opcodes_into_list)\n",
    "def get_opcode_pos(data_into_list,opcodes_into_list):\n",
    "    opcode_position = []\n",
    "    nx = 0\n",
    "    for x in data_into_list:\n",
    "        ny = 0\n",
    "        for y in opcodes_into_list:\n",
    "            if x == y:\n",
    "                opcode_position.append(ny)\n",
    "                break\n",
    "            ny+=1\n",
    "        nx+=1\n",
    "        # index for unknown opcodes\n",
    "        if len(opcodes_into_list) == ny:\n",
    "            opcode_position.append(ny)\n",
    "    return opcode_position\n",
    "\n",
    "# get opcode list from opdict\n",
    "def get_opcode_dict(malware_fam,file_path):\n",
    "    my_opcodes = file_path + 'opdict' + malware_fam + '.json'\n",
    "    with open(my_opcodes) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Real Malware Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing real malware../malware_data/Mixed/0009d99691e8eed99c7dd1500e07cda336d54260.asm.txt\n",
      "Processing real malware../malware_data/Mixed/00113d9802cca3deba19cf9daa17f1c2269de2b8.asm.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/002c3a4a12eb9cdc80754e4cddccbc98e5769392.asm.txt\n",
      "Processing real malware../malware_data/Mixed/0036d720d8ff6c8f4860b5c69deba7c400e4d356.asm.txt\n",
      "Processing real malware../malware_data/Mixed/0037c7716f1dc8e5c4e1f9a9f3e9d5aedb7a6979.asm.txt\n",
      "Processing real malware../malware_data/Mixed/003824de7a82d2db9fc877c44ea93f76dd0e5ca9.asm.txt\n",
      "Processing real malware../malware_data/Mixed/003c10125d80ba6cdbb05bc9aa047c7dbaa6b7ff.asm.txt\n",
      "Processing real malware../malware_data/Mixed/00427746e03afb4d3b28791a82315e52acf66a0b.asm.txt\n",
      "Processing real malware../malware_data/Mixed/00475573fe331e89916a27a9207f446bacfcf96a.asm.txt\n",
      "Processing real malware../malware_data/Mixed/004bb59ba37917bfea49e6904f0551df7b3c719f.asm.txt\n",
      "Processing real malware../malware_data/Mixed/005150c72b9cd08a62bc0d730e3593b4f160534a.asm.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/005231177f706856a0617a2c871d627ddedf54a7.asm.txt\n",
      "Processing real malware../malware_data/Mixed/005e219370aaf892509b509b76cb082b36b16fcf.asm.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/008e12ef58953dd19b9ca76d9db646e98b5b8d83.asm.txt\n",
      "Processing real malware../malware_data/Mixed/00b196f09c1901bdd0d3037eaf43e2df2c03c072.asm.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/00b6b7e3b923861ef8c257aa3803a239ce4d6154.asm.txt\n",
      "Processing real malware../malware_data/Mixed/00e1b5fbe988040bc1d8d730a6409b191c9d44d3.asm.txt\n",
      "Processing real malware../malware_data/Mixed/00eef206ff2380e704dfb958cd35260453d3fe65.asm.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/0119b382f053ba4321df18146a0e487eba79fc06.asm.txt\n",
      "Processing real malware../malware_data/Mixed/011ebcc5b5456c5de5657f094d67743ac19686c5.asm.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_0001728cd893bb5b9a559f13ef7cccd9.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_003b5b2ec5b60c30cc812ac448272469.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_005fe44b3738253b9cb0ac02b771d239.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_00803e68a890e89030032b1638bf8c39.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_00c91839c8081133dfdfb3249f60b984.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_013584e9d72881ba78d1c0f2ea844dee.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/VirusShare_013a88d3058686a0d649e11e631c01cb.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_015a47f45b2f76e8457bd560c5037404.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_016d977638ffe30d8defc82856e4de74.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_01ae72cb1dd0eedd5132400eef897a32.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_01d33f1c92479af3fa4aebda734c2bdf.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/VirusShare_01da6bfb8baff5138e19e7d96f1e3c90.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_01e7dd09637192cfde84a33117a19764.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_01fab32677bc11978b78687520d01edc.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_022e9bdb20216be2f12db06cbb024dc3.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_025fb66bc37d5988c44161ac3367433b.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_0318777113e208361577af0f63a9c24f.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_032bfc7d5066eec379e32f860393b66a.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_0359a684f82f126361577db419ffc63b.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_0363aac9cfde6d738da4b9f08d298417.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_038b35f52afedfb80d493b2786f8a34d.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_0395e2d26dcc3f3f9fe6e47398e1fa33.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_03a55bb5d74f398235c9158529c9904f.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_03bfe43767e0ef0f4c25be7e1c194e99.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_03cd5df7c100ed3cb703c940c3e0a712.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_03d6a067416e684cba893542c4ff1094.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_0403e161610d40dc1aa4a46d38ee9f97.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_040a43a53026d1eb8a1b39fb1216e716.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_04171a371492604e12b664788e91b7b1.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_04370eb0f8bcc4bc5da430cfb2ce2b5d.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_04797421ead1e1756222bc6e5663494d.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_047aa161639aa6729ec5c63b7ca1c065.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_04b48886b7083dfa82653efbfec002be.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_04dbdcb8f9634289f70e49718a508090.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/VirusShare_0590ab7140216666b7ffa812d7dbec59.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/VirusShare_05ee24bf351e24e2f6217fbde5ecfa81.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_0613b3bda683c88e3232da4c4605fae8.txt\n",
      "------------> is less than 600 ... skipping\n",
      "Processing real malware../malware_data/Mixed/VirusShare_0659968d923826d8b9755c81fd2adde5.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_07c88839c083ddf7ecb11e7bfde38ea8.txt\n",
      "Processing real malware../malware_data/Mixed/VirusShare_07dff17142e7339e6804dda527ef72fd.txt\n",
      "There are 50 malware files\n"
     ]
    }
   ],
   "source": [
    "# initialize variables\n",
    "dataset = []\n",
    "dataset_names = []\n",
    "dataset_ind = []\n",
    "\n",
    "# opening list of unique opcodes\n",
    "if unique_opcodes == 0:\n",
    "    my_fp = './code/visualize_explore/opcodes/'\n",
    "    my_opcodes = open(my_fp + 'opcodes' + real_malware + '.txt','r')\n",
    "    opcodes = my_opcodes.read()\n",
    "    opcodes_into_list = opcodes.replace('\\n', ' ').split(\" \")\n",
    "    opcodes_into_list.remove('')\n",
    "    # close opcodes file\n",
    "    my_opcodes.close()\n",
    "elif unique_opcodes == 1:\n",
    "    my_fp = main_dir + '/code-20230116T073801Z-001/code/fake_tests/opdicts/'\n",
    "    opcodes_into_list = get_opcode_dict(real_malware,my_fp)\n",
    "\n",
    "# real malware parameters\n",
    "my_filepath = \"../malware_data/\" + real_malware +'/'\n",
    "dir_list = os.listdir(my_filepath)\n",
    "\n",
    "# process real malware\n",
    "for fm in dir_list:\n",
    "    # if f == 'VirusShare_07c88839c083ddf7ecb11e7bfde38ea8.txt': # debug\n",
    "    print('Processing real malware' + my_filepath + fm)\n",
    "\n",
    "    # opening the file in read mode\n",
    "    my_file = open(my_filepath + fm, \"r\")\n",
    "\n",
    "    # reading the file\n",
    "    data = my_file.read()\n",
    "\n",
    "    # replacing end of line('/n') with ' ' and\n",
    "    # splitting the text it further when '.' is seen.\n",
    "    data_into_list = data.replace('\\n', ' ').split(\" \")\n",
    "    \n",
    "    # remove '' from opcodes\n",
    "    data_into_list.remove('')\n",
    "    \n",
    "    opcode_frequency = []\n",
    "    if len(data_into_list) >= max_sequence_length: # include malware with at least max_sequence_length number of opcodes\n",
    "            \n",
    "        # create (opcode frequency, rank) tuple\n",
    "        idx = 0\n",
    "        for x in opcodes_into_list:\n",
    "            count = countX(data_into_list, x)\n",
    "            #print('{} has occurred {} times'.format(x,count))\n",
    "            opcode_frequency.append((count,idx))\n",
    "            idx+=1\n",
    "        # print(sorted(opcode_frequency,reverse=True))\n",
    "\n",
    "        # opcodes are indexed by overall frequency\n",
    "        opcode_frequency = get_opcode_freq(opcode_frequency)\n",
    "\n",
    "        # opcodes are indexed by opcodes_into_list position\n",
    "        opcode_position = get_opcode_pos(data_into_list,opcodes_into_list)\n",
    "\n",
    "        # add real malware to dataset\n",
    "        dataset.append(opcode_position)\n",
    "        dataset_names.append(fm)\n",
    "        dataset_ind.append(0) # 0 is indicator for real malware\n",
    "    else:\n",
    "        # skip processing if malware file is empty\n",
    "        print('------------> is less than ' + str(max_sequence_length) + ' ... skipping')\n",
    "\n",
    "    # close file\n",
    "    my_file.close()\n",
    "\n",
    "# convert dataset to numpy arrays\n",
    "X = np.array(dataset,dtype=object)\n",
    "y = np.array(dataset_ind,dtype='int64')\n",
    "\n",
    "# print\n",
    "nSamples = len(X)\n",
    "print('There are', nSamples, 'malware files')\n",
    "\n",
    "# write real malware to images\n",
    "if write_opcode_images:\n",
    "    print('Writing real malware images')\n",
    "    tmp = sequence.pad_sequences(X, maxlen=max_sequence_length, dtype='int32',\n",
    "        padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "    for idx,img in enumerate(tmp):\n",
    "        plt.matshow(np.array(img).reshape(20,30))\n",
    "        ax = plt.gca()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.savefig('./results/' + 'real_malware_images/' + savedir_malware + '/' + 'real_' + real_malware + '_' + str(idx+1) + '.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_sequence_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_sequence_length)\n",
    "\n",
    "# reshape and normalize\n",
    "norm_factor = 1/(len(opcodes_into_list)+1)\n",
    "sample_size = X_train.shape[0]\n",
    "time_steps = X_train.shape[1]\n",
    "X_train_reshaped = X_train.reshape(sample_size, time_steps)*norm_factor\n",
    "\n",
    "sample_size = X_test.shape[0]\n",
    "time_steps = X_test.shape[1]\n",
    "X_test_reshaped = X_test.reshape(sample_size, time_steps)*norm_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create a function, which we will use to randomly sample from latent space distribution\n",
    "# Note, epsilon is sampled from a standard normal distribution and is used to maintain the required stochasticity of Z\n",
    "# Meanwhile, z-mean and z-sigma remain deterministic allowing the loss to backpropagate through the layers.\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder-Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Encoder-Input-Layer (InputLaye  [(None, 600)]       0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " Encoder-Hidden-Layer-1 (Dense)  (None, 64)          38464       ['Encoder-Input-Layer[0][0]']    \n",
      "                                                                                                  \n",
      " Encoder-Hidden-Layer-2 (Dense)  (None, 16)          1040        ['Encoder-Hidden-Layer-1[0][0]'] \n",
      "                                                                                                  \n",
      " Encoder-Hidden-Layer-3 (Dense)  (None, 8)           136         ['Encoder-Hidden-Layer-2[0][0]'] \n",
      "                                                                                                  \n",
      " Z-Mean (Dense)                 (None, 2)            18          ['Encoder-Hidden-Layer-3[0][0]'] \n",
      "                                                                                                  \n",
      " Z-Log-Sigma (Dense)            (None, 2)            18          ['Encoder-Hidden-Layer-3[0][0]'] \n",
      "                                                                                                  \n",
      " Z-Sampling-Layer (Lambda)      (None, 2)            0           ['Z-Mean[0][0]',                 \n",
      "                                                                  'Z-Log-Sigma[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 39,676\n",
      "Trainable params: 39,676\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ********** Create Encoder **********\n",
    "\n",
    "#--- Input Layer \n",
    "visible = keras.Input(shape=(max_sequence_length,), name='Encoder-Input-Layer')\n",
    "\n",
    "#--- Hidden Layer\n",
    "h_enc1 = Dense(units=64, activation='relu', name='Encoder-Hidden-Layer-1')(visible)\n",
    "h_enc2 = Dense(units=16, activation='relu', name='Encoder-Hidden-Layer-2')(h_enc1)\n",
    "h_enc3 = Dense(units=8, activation='relu', name='Encoder-Hidden-Layer-3')(h_enc2)\n",
    "\n",
    "#--- Custom Latent Space Layer\n",
    "z_mean = Dense(units=latent_dim, name='Z-Mean')(h_enc3) # Mean component\n",
    "z_log_sigma = Dense(units=latent_dim, name='Z-Log-Sigma')(h_enc3) # Standard deviation component\n",
    "z = Lambda(sampling, name='Z-Sampling-Layer')([z_mean, z_log_sigma]) # Z sampling layer\n",
    "\n",
    "#--- Create Encoder model\n",
    "encoder = Model(visible, [z_mean, z_log_sigma, z], name='Encoder-Model')\n",
    "\n",
    "# Display model diagram\n",
    "encoder.summary()\n",
    "#plot_model(encoder, show_shapes=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Decoder-Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input-Z-Sampling (InputLaye  [(None, 2)]              0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " Decoder-Hidden-Layer-1 (Den  (None, 8)                24        \n",
      " se)                                                             \n",
      "                                                                 \n",
      " Decoder-Hidden-Layer-2 (Den  (None, 16)               144       \n",
      " se)                                                             \n",
      "                                                                 \n",
      " Decoder-Hidden-Layer-3 (Den  (None, 64)               1088      \n",
      " se)                                                             \n",
      "                                                                 \n",
      " Decoder-Output-Layer (Dense  (None, 600)              39000     \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,256\n",
      "Trainable params: 40,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ********** Create Decoder **********\n",
    "\n",
    "#--- Input Layer \n",
    "latent_inputs = Input(shape=(latent_dim,), name='Input-Z-Sampling')\n",
    "\n",
    "#--- Hidden Layer\n",
    "h_dec = Dense(units=8, activation='relu', name='Decoder-Hidden-Layer-1')(latent_inputs)\n",
    "h_dec2 = Dense(units=16, activation='relu', name='Decoder-Hidden-Layer-2')(h_dec)\n",
    "h_dec3 = Dense(units=64, activation='relu', name='Decoder-Hidden-Layer-3')(h_dec2)\n",
    "\n",
    "#--- Output Layer\n",
    "outputs = Dense(max_sequence_length, activation='sigmoid', name='Decoder-Output-Layer')(h_dec3)\n",
    "\n",
    "#--- Create Decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='Decoder-Model')\n",
    "\n",
    "# Display model diagram\n",
    "decoder.summary()\n",
    "#plot_model(decoder, show_shapes=True, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outputs from a VAE model by specifying how the encoder-decoder models are linked\n",
    "outpt = decoder(encoder(visible)[2]) # note, outputs available from encoder model are z_mean, z_log_sigma and z. We take z by specifying [2]\n",
    "# Instantiate a VAE model\n",
    "vae = Model(inputs=visible, outputs=outpt, name='VAE-Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define VAE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction loss compares inputs and outputs and tries to minimise the difference\n",
    "r_loss = max_sequence_length * keras.losses.mse(visible, outpt)  # use MSE\n",
    "\n",
    "# KL divergence loss compares the encoded latent distribution Z with standard Normal distribution and penalizes if it's too different\n",
    "kl_loss =  -0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis = 1)\n",
    "\n",
    "# The VAE loss is a combination of reconstruction loss and KL loss\n",
    "vae_loss = K.mean(r_loss + kl_loss)\n",
    "\n",
    "# Add loss to the model and compile it\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the VAE and Plot Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using trained weights\n"
     ]
    }
   ],
   "source": [
    "if use_trained_weights:\n",
    "    print('Using trained weights')\n",
    "    load_status = vae.load_weights(fp + 'weights/'+fake_malware+'/my_weights_'+real_malware+'.h5')\n",
    "else:\n",
    "    # Train VAE model\n",
    "    history = vae.fit(X_train_reshaped, y_train, epochs=num_epochs, batch_size=num_batch_size, validation_data=(X_test_reshaped, y_test))\n",
    "\n",
    "    # Plot a loss chart\n",
    "    fig, ax = plt.subplots(figsize=(16,9), dpi=300)\n",
    "    plt.title(label='Model Loss by Epoch', loc='center')\n",
    "\n",
    "    ax.plot(history.history['loss'], label='Training Data', color='black')\n",
    "    ax.plot(history.history['val_loss'], label='Validation Data', color='red')\n",
    "    ax.set(xlabel='Epoch', ylabel='Loss')\n",
    "    plt.xticks(ticks=np.arange(len(history.history['loss']), step=5), labels=np.arange(0, len(history.history['loss']), step=5))\n",
    "    plt.legend()\n",
    "\n",
    "    # save loss chart\n",
    "    print('Saved loss chart')\n",
    "    plt.savefig(fp+'my_loss_chart_'+real_malware+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    # save weights\n",
    "    print('Saved weights')\n",
    "    vae.save_weights(fp+'weights/'+fake_malware+'/my_weights_'+real_malware+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latent space chart\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a,b,c = encoder.predict(X_train_reshaped)\n",
    "embeddings = sampling([a, b])\n",
    "\n",
    "mu = np.mean(a)\n",
    "sig = np.sqrt(abs(np.mean(b)))\n",
    "\n",
    "figsize = 12\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(embeddings[:, 0] , embeddings[:, 1], alpha=0.5, s=50)\n",
    "plt.xlabel(\"Dimension-1\", size=20)\n",
    "plt.ylabel(\"Dimension-2\", size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=20)\n",
    "plt.title(\"Projection of 2D Latent-Space for Dense VAE (\" + real_malware[0] +\")\", size=20)\n",
    "\n",
    "# save latent space chart\n",
    "print('Saved latent space chart')\n",
    "plt.savefig(fp+'my_latent_chart_'+real_malware+'.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Disabled] Another Way to Visualize Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use encoder model to encode inputs into a latent space\n",
    "#X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "# Recall that our encoder returns 3 arrays: z-mean, z-log-sigma and z. We plot the values for z\n",
    "# Create a scatter plot\n",
    "#fig = px.scatter(None, x=X_test_encoded[2][:,0], y=X_test_encoded[2][:,1], \n",
    "#                 opacity=1, color=y_test.astype(str))\n",
    "\n",
    "# Change chart background color\n",
    "#fig.update_layout(dict(plot_bgcolor = 'white'))\n",
    "\n",
    "# Update axes lines\n",
    "#fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='white', \n",
    "#                 zeroline=True, zerolinewidth=1, zerolinecolor='white', \n",
    "#                 showline=True, linewidth=1, linecolor='white',\n",
    "#                 title_font=dict(size=10), tickfont=dict(size=10))\n",
    "#\n",
    "#fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='white', \n",
    "#                 zeroline=True, zerolinewidth=1, zerolinecolor='white', \n",
    "#                 showline=True, linewidth=1, linecolor='white',\n",
    "#                 title_font=dict(size=10), tickfont=dict(size=10))\n",
    "\n",
    "# Set figure title\n",
    "#fig.update_layout(title_text=\"MNIST digit representation in the 2D Latent Space\")\n",
    "\n",
    "# Update marker size\n",
    "#fig.update_traces(marker=dict(size=2))\n",
    "\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Fake Malware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nF in range(nSamples):\n",
    "    randomlist = []\n",
    "    for i in range(0,latent_dim):\n",
    "        #n = random.randint(0,len(opcodes_into_list))\n",
    "        #n = (random.random() - 0.5)*2\n",
    "        #n = random.random()\n",
    "        #n = 2*random.random()-1\n",
    "        n = np.random.normal(mu, sig)\n",
    "        randomlist.append(n)\n",
    "    z_sample_opcode=[randomlist]\n",
    "\n",
    "    # Decode latent inputs (i.e., generate new outputs)\n",
    "    opcode_decoded = decoder.predict(z_sample_opcode)\n",
    "\n",
    "    tmp = np.rint(opcode_decoded.reshape(20,30)*len(opcodes_into_list)).astype(int)\n",
    "\n",
    "    if write_opcode_files:\n",
    "    # write opcode integers\n",
    "        my_opcodes = open(fp + 'samples' + '/' + fake_malware + '/' + 'generated_' + real_malware + '_' + str(nF+1) + '.txt','a+')\n",
    "\n",
    "        for row in tmp:\n",
    "            for col in row:\n",
    "                my_opcodes.write(str(col))\n",
    "                my_opcodes.write('\\n')\n",
    "        # close opcodes file\n",
    "        my_opcodes.close()\n",
    "\n",
    "    if write_opcode_images:\n",
    "        # Reshape and display/save the image\n",
    "        plt.matshow(tmp)\n",
    "        ax = plt.gca()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.savefig(fp + 'images/' + savedir_malware + '/' + 'generated_' + real_malware + '_' + str(nF+1) + '.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # append to fake malware to dataset\n",
    "    dataset.append(tmp.reshape(1,max_sequence_length)[0])\n",
    "    dataset_ind.append(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(X, y, real_malware, target_names, max_sequence_length, embedding_vector_length, num_unique, classify_test_size):\n",
    "    classifier_name = 'LSTM'\n",
    "    print('******%s******' % classifier_name)\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X = sequence.pad_sequences(X, maxlen=max_sequence_length)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=classify_test_size, shuffle=True)\n",
    "    clf = Sequential()\n",
    "    clf.add(Embedding(num_unique, embedding_vector_length, input_length=max_sequence_length))\n",
    "    clf.add(LSTM(100))\n",
    "    clf.add(Dense(1, activation='sigmoid'))\n",
    "    clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.01, patience=10, restore_best_weights=True)\n",
    "    clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback], verbose=1)\n",
    "    scores = clf.evaluate(X_test, y_test, verbose=1)\n",
    "    accuracy = scores[1]\n",
    "    y_pred = np.round(clf.predict(X_test))\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall= recall_score(y_test, y_pred)\n",
    "\n",
    "    show_confusion_matrix = False\n",
    "    if show_confusion_matrix:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.array(target_names), cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title('%s Confusion Matrix \\n using %s'%(real_malware,classifier_name))\n",
    "        plt.show()\n",
    "    \n",
    "    #class_report = classification_report(y_test, y_pred, target_names=target_names) \n",
    "\n",
    "    return accuracy, precision, recall#, class_report\n",
    "\n",
    "def lstm_kfold(X, y, real_malware, target_names, max_sequence_length, embedding_vector_length, num_unique, classify_test_size):\n",
    "    classifier_name = 'Kfold LSTM'\n",
    "    print('******%s******' % classifier_name)\n",
    "    k = int(1/classify_test_size)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    best = 0.\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=18245)\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X = sequence.pad_sequences(X, maxlen=max_sequence_length)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # build LSTM model\n",
    "        clf = Sequential()\n",
    "        clf.add(Embedding(num_unique, embedding_vector_length, input_length=max_sequence_length))\n",
    "        clf.add(LSTM(100))\n",
    "        clf.add(Dense(1, activation='sigmoid'))\n",
    "        clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   \n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.01, patience=10, restore_best_weights=True)\n",
    "        clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback], verbose=1)\n",
    "        scores = clf.evaluate(X_test, y_test, verbose=1)\n",
    "        acc = scores[1]\n",
    "        accuracy.append(acc)\n",
    "        if acc>best:\n",
    "            best = acc\n",
    "            best_train, best_test = train_index, test_index\n",
    "            \n",
    "        y_pred = np.round(clf.predict(X_test))\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        \n",
    "    print('\\n%s accuracy: ' % classifier_name, accuracy)\n",
    "    print('%s precision: ' % classifier_name, precision)\n",
    "    print('%s recall: ' % classifier_name, recall)\n",
    "    \n",
    "    print('\\nAvg %s accuracy: %0.2f'% (classifier_name, np.mean(np.array(accuracy))))\n",
    "    print('Avg %s precision: %0.2f'% (classifier_name, np.mean(np.array(precision))))\n",
    "    print('Avg %s recall: %0.2f\\n'% (classifier_name ,np.mean(np.array(recall))))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[best_train], X[best_test], y[best_train], y[best_test]\n",
    "    clf = Sequential()\n",
    "    clf.add(Embedding(num_unique, embedding_vector_length, input_length=max_sequence_length))\n",
    "    clf.add(LSTM(100))\n",
    "    clf.add(Dense(1, activation='sigmoid'))\n",
    "    clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.01, patience=10, restore_best_weights=True)\n",
    "    clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback], verbose=1)\n",
    "    y_pred = np.round(clf.predict(X_test))\n",
    "    \n",
    "    show_confusion_matrix = False\n",
    "    if show_confusion_matrix:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.array(target_names), cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title('%s Confusion Matrix \\n using %s'%(real_malware,classifier_name))\n",
    "        plt.show()\n",
    "    \n",
    "    #class_report = classification_report(y_test, y_pred, target_names=target_names) \n",
    "\n",
    "    return np.mean(np.array(accuracy)), np.mean(np.array(precision)), np.mean(np.array(recall))#, class_report\n",
    "\n",
    "def bi_lstm(X, y, real_malware, target_names, max_sequence_length, embedding_vector_length, num_unique, classify_test_size):\n",
    "    classifier_name = 'Bidirectional_LSTM'\n",
    "    print('******%s******' % classifier_name)\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X = sequence.pad_sequences(X, maxlen=max_sequence_length)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=classify_test_size, shuffle=True)\n",
    "    clf = Sequential()\n",
    "    clf.add(Embedding(num_unique, embedding_vector_length, input_length=max_sequence_length))\n",
    "    #clf.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    clf.add(Bidirectional(LSTM(100, dropout=0.2)))\n",
    "    clf.add(Dense(1, activation='sigmoid'))\n",
    "    clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.01, patience=10, restore_best_weights=True)\n",
    "    clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback], verbose=1)\n",
    "    scores = clf.evaluate(X_test, y_test, verbose=1)\n",
    "    accuracy = scores[1]\n",
    "    y_pred = np.round(clf.predict(X_test))\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall= recall_score(y_test, y_pred)\n",
    "\n",
    "    show_confusion_matrix = False\n",
    "    if show_confusion_matrix:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.array(target_names), cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title('%s Confusion Matrix \\n using %s'%(real_malware,classifier_name))\n",
    "        plt.show()\n",
    "    \n",
    "    #class_report = classification_report(y_test, y_pred, target_names=target_names) \n",
    "\n",
    "    return accuracy, precision, recall#, class_report\n",
    "\n",
    "def bi_lstm_kfold(X, y, real_malware, target_names,max_sequence_length, embedding_vector_length, num_unique, classify_test_size):\n",
    "    classifier_name = 'Kfold Bidirectional_LSTM'\n",
    "    print('******%s******' % classifier_name)\n",
    "    k = int(1/classify_test_size)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    best = 0.\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=18245)\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X = sequence.pad_sequences(X, maxlen=max_sequence_length)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # build LSTM model\n",
    "        clf = Sequential()\n",
    "        clf.add(Embedding(num_unique, embedding_vector_length, input_length=max_sequence_length))\n",
    "        #clf.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "        clf.add(Bidirectional(LSTM(100, dropout=0.2)))\n",
    "        clf.add(Dense(1, activation='sigmoid'))\n",
    "        clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.01, patience=10, restore_best_weights=True)\n",
    "        clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback], verbose=1)\n",
    "        scores = clf.evaluate(X_test, y_test, verbose=1)\n",
    "        acc = scores[1]\n",
    "        accuracy.append(acc)\n",
    "        if acc>best:\n",
    "            best = acc\n",
    "            best_train, best_test = train_index, test_index\n",
    "            \n",
    "        y_pred = np.round(clf.predict(X_test))\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        \n",
    "    print('\\n%s accuracy: ' % classifier_name, accuracy)\n",
    "    print('%s precision: ' % classifier_name, precision)\n",
    "    print('%s recall: ' % classifier_name, recall)\n",
    "    \n",
    "    print('\\nAvg %s accuracy: %0.2f'% (classifier_name, np.mean(np.array(accuracy))))\n",
    "    print('Avg %s precision: %0.2f'% (classifier_name, np.mean(np.array(precision))))\n",
    "    print('Avg %s recall: %0.2f\\n'% (classifier_name ,np.mean(np.array(recall))))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[best_train], X[best_test], y[best_train], y[best_test]\n",
    "    clf = Sequential()\n",
    "    clf.add(Embedding(num_unique, embedding_vector_length, input_length=max_sequence_length))\n",
    "    #clf.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    clf.add(Bidirectional(LSTM(100, dropout=0.2)))\n",
    "    clf.add(Dense(1, activation='sigmoid'))\n",
    "    clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.01, patience=10, restore_best_weights=True)\n",
    "    clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback], verbose=1)\n",
    "    y_pred = np.round(clf.predict(X_test))\n",
    "    \n",
    "    show_confusion_matrix = False\n",
    "    if show_confusion_matrix:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.array(target_names), cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title('%s Confusion Matrix \\n using %s'%(real_malware,classifier_name))\n",
    "        plt.show()\n",
    "    \n",
    "    #class_report = classification_report(y_test, y_pred, target_names=target_names) \n",
    "\n",
    "    return np.mean(np.array(accuracy)), np.mean(np.array(precision)), np.mean(np.array(recall))#, class_report\n",
    "\n",
    "def cnn_lstm(X, y, real_malware, target_names, max_sequence_length, embedding_vector_length, num_unique, classify_test_size):\n",
    "    classifier_name = 'CNN_LSTM'\n",
    "    print('******%s******' % classifier_name)\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X = sequence.pad_sequences(X, maxlen=max_sequence_length)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=classify_test_size, shuffle=True)\n",
    "    clf = Sequential()\n",
    "    clf.add(Embedding(num_unique, embedding_vector_length, input_length=max_sequence_length))\n",
    "    clf.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    clf.add(MaxPooling1D(pool_size=2))\n",
    "    clf.add(LSTM(100))\n",
    "    clf.add(Dense(1, activation='sigmoid'))\n",
    "    clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.01, patience=10, restore_best_weights=True)\n",
    "    clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback], verbose=1)\n",
    "    scores = clf.evaluate(X_test, y_test, verbose=1)\n",
    "    accuracy = scores[1]\n",
    "    y_pred = np.round(clf.predict(X_test))\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall= recall_score(y_test, y_pred)\n",
    "\n",
    "    show_confusion_matrix = False\n",
    "    if show_confusion_matrix:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.array(target_names), cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title('%s Confusion Matrix \\n using %s'%(real_malware,classifier_name))\n",
    "        plt.show()\n",
    "    \n",
    "    #class_report = classification_report(y_test, y_pred, target_names=target_names) \n",
    "\n",
    "    return accuracy, precision, recall#, class_report\n",
    "\n",
    "def cnn_lstm_kfold(X, y, real_malware, target_names,max_sequence_length, embedding_vector_length, num_unique, classify_test_size):\n",
    "    classifier_name = 'Kfold CNN_LSTM'\n",
    "    print('******%s******' % classifier_name)\n",
    "    k = int(1/classify_test_size)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    best = 0.\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=18245)\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X = sequence.pad_sequences(X, maxlen=max_sequence_length)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # build LSTM model\n",
    "        clf = Sequential()\n",
    "        clf.add(Embedding(num_unique, embedding_vector_length, input_length=max_sequence_length))\n",
    "        clf.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "        clf.add(MaxPooling1D(pool_size=2))\n",
    "        clf.add(LSTM(100))\n",
    "        clf.add(Dense(1, activation='sigmoid'))\n",
    "        clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.01, patience=10, restore_best_weights=True)\n",
    "        clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback], verbose=1)\n",
    "        scores = clf.evaluate(X_test, y_test, verbose=1)\n",
    "        acc = scores[1]\n",
    "        accuracy.append(acc)\n",
    "        if acc>best:\n",
    "            best = acc\n",
    "            best_train, best_test = train_index, test_index\n",
    "            \n",
    "        y_pred = np.round(clf.predict(X_test))\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        \n",
    "    print('\\n%s accuracy: ' % classifier_name, accuracy)\n",
    "    print('%s precision: ' % classifier_name, precision)\n",
    "    print('%s recall: ' % classifier_name, recall)\n",
    "    \n",
    "    print('\\nAvg %s accuracy: %0.2f'% (classifier_name, np.mean(np.array(accuracy))))\n",
    "    print('Avg %s precision: %0.2f'% (classifier_name, np.mean(np.array(precision))))\n",
    "    print('Avg %s recall: %0.2f\\n'% (classifier_name ,np.mean(np.array(recall))))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[best_train], X[best_test], y[best_train], y[best_test]\n",
    "    clf = Sequential()\n",
    "    clf.add(Embedding(num_unique, embedding_vector_length, input_length=max_sequence_length))\n",
    "    clf.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    clf.add(MaxPooling1D(pool_size=2))\n",
    "    clf.add(LSTM(100))\n",
    "    clf.add(Dense(1, activation='sigmoid'))\n",
    "    clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.01, patience=10, restore_best_weights=True)\n",
    "    clf.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64, callbacks=[callback], verbose=1)\n",
    "    y_pred = np.round(clf.predict(X_test))\n",
    "    \n",
    "    show_confusion_matrix = False\n",
    "    if show_confusion_matrix:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.array(target_names), cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title('%s Confusion Matrix \\n using %s'%(real_malware,classifier_name))\n",
    "        plt.show()\n",
    "    \n",
    "    #class_report = classification_report(y_test, y_pred, target_names=target_names) \n",
    "\n",
    "    return np.mean(np.array(accuracy)), np.mean(np.array(precision)), np.mean(np.array(recall))#, class_report\n",
    "\n",
    "def support_vector_machine(X, y, real_malware, target_names, max_sequence_length, classify_test_size):\n",
    "    classifier_name = 'Support Vector Machines'\n",
    "    print('******%s******' % classifier_name)\n",
    "    k = int(1/classify_test_size)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    best = 0.\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X = sequence.pad_sequences(X, maxlen=max_sequence_length)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf = svm.SVC(C=5, kernel='rbf')\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc = clf.score(X_test, y_test)\n",
    "        accuracy.append(acc)\n",
    "        if acc>best:\n",
    "            best = acc\n",
    "            best_train, best_test = train_index, test_index\n",
    "            \n",
    "        y_pred = clf.predict(X_test)\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        \n",
    "    print('\\n%s accuracy: ' % classifier_name, accuracy)\n",
    "    print('%s precision: ' % classifier_name, precision)\n",
    "    print('%s recall: ' % classifier_name, recall)\n",
    "    \n",
    "    print('\\nAvg %s accuracy: %0.2f'% (classifier_name, np.mean(np.array(accuracy))))\n",
    "    print('Avg %s precision: %0.2f'% (classifier_name, np.mean(np.array(precision))))\n",
    "    print('Avg %s recall: %0.2f\\n'% (classifier_name ,np.mean(np.array(recall))))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[best_train], X[best_test], y[best_train], y[best_test]\n",
    "    clf = svm.SVC(C=5, kernel='rbf')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    show_confusion_matrix = False\n",
    "    if show_confusion_matrix:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.array(target_names), cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title('%s Confusion Matrix \\n using %s'%(real_malware,classifier_name))\n",
    "        plt.show()\n",
    "    \n",
    "    #class_report = classification_report(y_test, y_pred, target_names=target_names) \n",
    "\n",
    "    return np.mean(np.array(accuracy)), np.mean(np.array(precision)), np.mean(np.array(recall))#, class_report\n",
    "\n",
    "def random_forest(X, y, real_malware, target_names, max_sequence_length, classify_test_size):\n",
    "    classifier_name = 'Random Forest'\n",
    "    print('******%s******' % classifier_name)\n",
    "    k = int(1/classify_test_size)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    best = 0.\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=6357)\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X = sequence.pad_sequences(X, maxlen=max_sequence_length)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc = clf.score(X_test, y_test)\n",
    "        accuracy.append(acc)\n",
    "        if acc>best:\n",
    "            best = acc\n",
    "            best_train, best_test = train_index, test_index\n",
    "            \n",
    "        y_pred = clf.predict(X_test)\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        \n",
    "    print('\\n%s accuracy: ' % classifier_name, accuracy)\n",
    "    print('%s precision: ' % classifier_name, precision)\n",
    "    print('%s recall: ' % classifier_name, recall)\n",
    "    \n",
    "    print('\\nAvg %s accuracy: %0.2f'% (classifier_name, np.mean(np.array(accuracy))))\n",
    "    print('Avg %s precision: %0.2f'% (classifier_name, np.mean(np.array(precision))))\n",
    "    print('Avg %s recall: %0.2f\\n'% (classifier_name ,np.mean(np.array(recall))))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[best_train], X[best_test], y[best_train], y[best_test]\n",
    "    clf = RandomForestClassifier(n_estimators=50, max_depth=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    show_confusion_matrix = False\n",
    "    if show_confusion_matrix:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.array(target_names), cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title('%s Confusion Matrix \\n using %s'%(real_malware,classifier_name))\n",
    "        plt.show()\n",
    "    \n",
    "    #class_report = classification_report(y_test, y_pred, target_names=target_names) \n",
    "\n",
    "    return np.mean(np.array(accuracy)), np.mean(np.array(precision)), np.mean(np.array(recall))#, class_report\n",
    "\n",
    "def k_nearest_neighbors(X, y, real_malware, target_names, max_sequence_length, classify_test_size):\n",
    "    classifier_name = 'K-Nearest Neighbors'\n",
    "    print('******%s******' % classifier_name)\n",
    "    k = int(1/classify_test_size)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    best = 0.\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=18245)\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X = sequence.pad_sequences(X, maxlen=max_sequence_length)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf = KNeighborsClassifier(n_neighbors=1, p=2)\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc = clf.score(X_test, y_test)\n",
    "        accuracy.append(acc)\n",
    "        if acc>best:\n",
    "            best = acc\n",
    "            best_train, best_test = train_index, test_index\n",
    "            \n",
    "        y_pred = clf.predict(X_test)\n",
    "        precision.append(precision_score(y_test, y_pred))\n",
    "        recall.append(recall_score(y_test, y_pred))\n",
    "        \n",
    "    print('\\n%s accuracy: ' % classifier_name, accuracy)\n",
    "    print('%s precision: ' % classifier_name, precision)\n",
    "    print('%s recall: ' % classifier_name, recall)\n",
    "    \n",
    "    print('\\nAvg %s accuracy: %0.2f'% (classifier_name, np.mean(np.array(accuracy))))\n",
    "    print('Avg %s precision: %0.2f'% (classifier_name, np.mean(np.array(precision))))\n",
    "    print('Avg %s recall: %0.2f\\n'% (classifier_name ,np.mean(np.array(recall))))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[best_train], X[best_test], y[best_train], y[best_test]\n",
    "    clf = KNeighborsClassifier(n_neighbors=1, p=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    show_confusion_matrix = False\n",
    "    if show_confusion_matrix:\n",
    "        fig, ax = plt.subplots(figsize=(4,4))\n",
    "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=np.array(target_names), cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title('%s Confusion Matrix \\n using %s'%(real_malware,classifier_name))\n",
    "        plt.show()\n",
    "    \n",
    "    #class_report = classification_report(y_test, y_pred, target_names=target_names) \n",
    "\n",
    "    return np.mean(np.array(accuracy)), np.mean(np.array(precision)), np.mean(np.array(recall))#, class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify Real and Fake Malware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(dataset, dataset_ind ,mw_classify_size):\n",
    "    # convert dataset to numpy arrays\n",
    "    X_classify = np.array(dataset,dtype=object)\n",
    "    y_classify = np.array(dataset_ind,dtype='int64')\n",
    "    \n",
    "    # choose malware sample subset for training\n",
    "    mw_classify_idx = np.random.permutation(len(X_classify))[:mw_classify_size]\n",
    "    X_train = X_classify[mw_classify_idx]\n",
    "    y_train = y_classify[mw_classify_idx]\n",
    "\n",
    "    # truncate and pad input sequences\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=max_sequence_length)\n",
    "\n",
    "    # create LSTM model\n",
    "\n",
    "    if use_deep_classifiers:\n",
    "\n",
    "        if use_kfold_deep_classifiers:\n",
    "            # Kfold LSTM classifiers\n",
    "            print('Running Kfold Deep Classifiers')\n",
    "            lstm_accuracy, lstm_precision, lstm_recall = lstm_kfold(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "            bi_lstm_accuracy, bi_lstm_precision, bi_lstm_recall = bi_lstm_kfold(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "            cnn_lstm_accuracy, cnn_lstm_precision, cnn_lstm_recall = cnn_lstm_kfold(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "        else:\n",
    "            # Standard LSTM classifiers\n",
    "            print('Running Deep Classifiers')\n",
    "            lstm_accuracy, lstm_precision, lstm_recall = lstm(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "            bi_lstm_accuracy, bi_lstm_precision, bi_lstm_recall = bi_lstm(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "            cnn_lstm_accuracy, cnn_lstm_precision, cnn_lstm_recall = cnn_lstm(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "\n",
    "    svm_accuracy, svm_precision, svm_recall = support_vector_machine(X_train, y_train, real_malware, target_names, max_sequence_length, classify_test_size)\n",
    "    rf_accuracy, rf_precision, rf_recall = random_forest(X_train, y_train, real_malware, target_names, max_sequence_length, classify_test_size)\n",
    "    knn_accuracy, knn_precision, knn_recall = k_nearest_neighbors(X_train, y_train, real_malware, target_names, max_sequence_length, classify_test_size)\n",
    "    \n",
    "    print('Classification on ' + str(int(classify_test_size*len(X_train))) + ' Authentic and Synthetic Dense VAE Samples from the ' + real_malware + ' Malware Family')\n",
    "    print('-------------------------------------------------------------')\n",
    "\n",
    "    print(\"                               Accuracy    Precison    Recall\")\n",
    "    print(\"SVM Score:                     %0.2f        %0.2f        %0.2f  \" % (svm_accuracy, svm_precision, svm_recall))\n",
    "    print(\"Random Forest Score:           %0.2f        %0.2f        %0.2f  \" % (rf_accuracy, rf_precision, rf_recall))\n",
    "    print(\"k-Nearest Neighbor Score:      %0.2f        %0.2f        %0.2f  \" % (knn_accuracy, knn_precision, knn_recall))\n",
    "\n",
    "    if use_deep_classifiers:\n",
    "        print(\"Standard LSTM Score:           %0.2f        %0.2f        %0.2f  \" % (lstm_accuracy, lstm_precision, lstm_recall))\n",
    "        print(\"Bidirectional LSTM Score:      %0.2f        %0.2f        %0.2f  \" % (bi_lstm_accuracy, bi_lstm_precision, bi_lstm_recall))\n",
    "        print(\"CNN LSTM Score:                %0.2f        %0.2f        %0.2f  \" % (cnn_lstm_accuracy, cnn_lstm_precision, cnn_lstm_recall))\n",
    "\n",
    "        list_accuracy = [svm_accuracy, rf_accuracy, knn_accuracy, lstm_accuracy, bi_lstm_accuracy, cnn_lstm_accuracy]\n",
    "        list_precision = [svm_precision, rf_precision, knn_precision, lstm_precision, bi_lstm_precision, cnn_lstm_precision]\n",
    "        list_recall = [svm_recall, rf_recall, knn_recall, lstm_recall, bi_lstm_recall, cnn_lstm_recall]\n",
    "    else:\n",
    "        list_accuracy = [svm_accuracy, rf_accuracy, knn_accuracy]\n",
    "        list_precision = [svm_precision, rf_precision, knn_precision]\n",
    "        list_recall = [svm_recall, rf_recall, knn_recall]\n",
    "    print(\"=============================================================\")\n",
    "    print(\"Average Score:                 %0.2f        %0.2f        %0.2f  \" % (sum(list_accuracy)/len(list_accuracy), sum(list_precision)/len(list_precision), sum(list_recall)/len(list_recall)))\n",
    "    print(\"=============================================================\")\n",
    "\n",
    "    return sum(list_accuracy)/len(list_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Deep Classifiers\n",
      "******LSTM******\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6948 - accuracy: 0.6250 - val_loss: 0.7002 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.6868 - accuracy: 0.7500 - val_loss: 0.7044 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 0.6790 - accuracy: 0.7500 - val_loss: 0.7089 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.6707 - accuracy: 0.7500 - val_loss: 0.7142 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.6614 - accuracy: 0.7500 - val_loss: 0.7204 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.6503 - accuracy: 0.7500 - val_loss: 0.7283 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.6365 - accuracy: 0.7500 - val_loss: 0.7387 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.6183 - accuracy: 0.7500 - val_loss: 0.7537 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.5925 - accuracy: 0.7500 - val_loss: 0.7779 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 0.5526 - accuracy: 0.7500 - val_loss: 0.8250 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.4818 - accuracy: 0.7500 - val_loss: 0.9722 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.3694 - accuracy: 0.7500 - val_loss: 2.4525 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.6481 - accuracy: 0.7500 - val_loss: 1.8159 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 0.5053 - accuracy: 0.7500 - val_loss: 0.8910 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.3223 - accuracy: 0.7500 - val_loss: 0.7855 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 0.3554 - accuracy: 0.7500 - val_loss: 0.7420 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.3807 - accuracy: 0.7500 - val_loss: 0.7145 - val_accuracy: 0.3333\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 0.3886 - accuracy: 0.8750 - val_loss: 0.6929 - val_accuracy: 0.3333\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.3839 - accuracy: 0.8750 - val_loss: 0.6734 - val_accuracy: 0.6667\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.3696 - accuracy: 1.0000 - val_loss: 0.6536 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.3468 - accuracy: 1.0000 - val_loss: 0.6320 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.3161 - accuracy: 1.0000 - val_loss: 0.6066 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.2788 - accuracy: 1.0000 - val_loss: 0.5757 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 0.2379 - accuracy: 1.0000 - val_loss: 0.5373 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 0.1983 - accuracy: 1.0000 - val_loss: 0.4910 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 0.1640 - accuracy: 1.0000 - val_loss: 0.4381 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 459ms/step - loss: 0.1358 - accuracy: 1.0000 - val_loss: 0.3829 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 0.1129 - accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 0.0944 - accuracy: 1.0000 - val_loss: 0.2838 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 1s 503ms/step - loss: 0.0796 - accuracy: 1.0000 - val_loss: 0.2445 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 0.0677 - accuracy: 1.0000 - val_loss: 0.2120 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 0.0582 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 0.1628 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.1442 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.0392 - accuracy: 1.0000 - val_loss: 0.1285 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 0.0349 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.0847 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 441ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 436ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0700 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0585 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0491 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0279 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0585 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e399410\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\e399410\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******Bidirectional_LSTM******\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6882 - accuracy: 0.8750 - val_loss: 0.6825 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.6803 - accuracy: 1.0000 - val_loss: 0.6763 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.6723 - accuracy: 1.0000 - val_loss: 0.6694 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 442ms/step - loss: 0.6646 - accuracy: 1.0000 - val_loss: 0.6610 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.6559 - accuracy: 1.0000 - val_loss: 0.6511 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 480ms/step - loss: 0.6441 - accuracy: 1.0000 - val_loss: 0.6386 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 0.6315 - accuracy: 1.0000 - val_loss: 0.6228 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 495ms/step - loss: 0.6132 - accuracy: 1.0000 - val_loss: 0.6017 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 0.5892 - accuracy: 1.0000 - val_loss: 0.5723 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.5610 - accuracy: 1.0000 - val_loss: 0.5279 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 495ms/step - loss: 0.5183 - accuracy: 1.0000 - val_loss: 0.4546 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 0.4476 - accuracy: 1.0000 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 0.3156 - accuracy: 1.0000 - val_loss: 0.1476 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.1494 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.0562 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 507ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.0769 - accuracy: 1.0000 - val_loss: 0.1782 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 0.1360 - accuracy: 1.0000 - val_loss: 0.1468 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 0.1118 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 455ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 447ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 444ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "******CNN_LSTM******\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6938 - accuracy: 0.6250 - val_loss: 0.7038 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.6878 - accuracy: 0.6250 - val_loss: 0.7133 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.6820 - accuracy: 0.6250 - val_loss: 0.7233 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.6757 - accuracy: 0.6250 - val_loss: 0.7342 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.6686 - accuracy: 0.6250 - val_loss: 0.7465 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.6599 - accuracy: 0.6250 - val_loss: 0.7610 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.6490 - accuracy: 0.6250 - val_loss: 0.7787 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.6350 - accuracy: 0.6250 - val_loss: 0.8017 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.6164 - accuracy: 0.6250 - val_loss: 0.8335 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.5906 - accuracy: 0.6250 - val_loss: 0.8822 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.5537 - accuracy: 0.6250 - val_loss: 0.9689 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.5020 - accuracy: 0.6250 - val_loss: 1.1923 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.4847 - accuracy: 0.6250 - val_loss: 1.0413 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.4241 - accuracy: 0.6250 - val_loss: 0.9049 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.3747 - accuracy: 0.6250 - val_loss: 0.8215 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.3317 - accuracy: 0.6250 - val_loss: 0.7586 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.2880 - accuracy: 0.6250 - val_loss: 0.6999 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.2578 - accuracy: 1.0000 - val_loss: 0.6450 - val_accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.2353 - accuracy: 1.0000 - val_loss: 0.5941 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.2160 - accuracy: 1.0000 - val_loss: 0.5464 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.1985 - accuracy: 1.0000 - val_loss: 0.5008 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.1819 - accuracy: 1.0000 - val_loss: 0.4569 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.1660 - accuracy: 1.0000 - val_loss: 0.4142 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.1506 - accuracy: 1.0000 - val_loss: 0.3730 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.1357 - accuracy: 1.0000 - val_loss: 0.3332 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.1213 - accuracy: 1.0000 - val_loss: 0.2950 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.1075 - accuracy: 1.0000 - val_loss: 0.2589 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0944 - accuracy: 1.0000 - val_loss: 0.2250 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0821 - accuracy: 1.0000 - val_loss: 0.1936 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0708 - accuracy: 1.0000 - val_loss: 0.1650 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.1392 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 0.1165 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.0968 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.0801 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.6601 - accuracy: 0.8750 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.6493 - accuracy: 0.8750 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.6280 - accuracy: 0.8750 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "******Support Vector Machines******\n",
      "\n",
      "Support Vector Machines accuracy:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Support Vector Machines precision:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Support Vector Machines recall:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Avg Support Vector Machines accuracy: 1.00\n",
      "Avg Support Vector Machines precision: 1.00\n",
      "Avg Support Vector Machines recall: 1.00\n",
      "\n",
      "******Random Forest******\n",
      "\n",
      "Random Forest accuracy:  [0.3333333333333333, 1.0, 1.0, 0.5, 0.5]\n",
      "Random Forest precision:  [0.3333333333333333, 1.0, 1.0, 0.5, 0.5]\n",
      "Random Forest recall:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Avg Random Forest accuracy: 0.67\n",
      "Avg Random Forest precision: 0.67\n",
      "Avg Random Forest recall: 1.00\n",
      "\n",
      "******K-Nearest Neighbors******\n",
      "\n",
      "K-Nearest Neighbors accuracy:  [0.6666666666666666, 0.0, 1.0, 0.0, 1.0]\n",
      "K-Nearest Neighbors precision:  [0.6666666666666666, 0.0, 1.0, 0.0, 1.0]\n",
      "K-Nearest Neighbors recall:  [1.0, 0.0, 1.0, 0.0, 1.0]\n",
      "\n",
      "Avg K-Nearest Neighbors accuracy: 0.53\n",
      "Avg K-Nearest Neighbors precision: 0.53\n",
      "Avg K-Nearest Neighbors recall: 0.60\n",
      "\n",
      "Classification on 2 Authentic and Synthetic Dense VAE Samples from the Mixed Malware Family\n",
      "-------------------------------------------------------------\n",
      "                               Accuracy    Precison    Recall\n",
      "SVM Score:                     1.00        1.00        1.00  \n",
      "Random Forest Score:           0.67        0.67        1.00  \n",
      "k-Nearest Neighbor Score:      0.53        0.53        0.60  \n",
      "Standard LSTM Score:           1.00        0.00        0.00  \n",
      "Bidirectional LSTM Score:      1.00        1.00        1.00  \n",
      "CNN LSTM Score:                1.00        1.00        1.00  \n",
      "=============================================================\n",
      "Average Score:                 0.87        0.70        0.77  \n",
      "=============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e399410\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\e399410\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# choose standard or min-sample classification\n",
    "run_min_samples = 0\n",
    "\n",
    "# standard classification parameters\n",
    "list_classifiers = ['all'] #['svm','rf','knn','lstm','bi-lstm','cnn-lstm', 'all']\n",
    "mw_classify_size = 50 # requested malware samples for classifier training (capped at number of samples used for VAE training)\n",
    "\n",
    "# min-sample classification parameters\n",
    "target_score = 0.95\n",
    "LOW_SAMPLE_BOUND = 10\n",
    "HIGH_SAMPLE_BOUND = mw_classify_size\n",
    "\n",
    "classifier_min_samples = []\n",
    "classifier_min_sample_scores = []\n",
    "\n",
    "for select_classifier in list_classifiers:\n",
    "\n",
    "    if run_classifiers:\n",
    "\n",
    "        current_score = 1.0\n",
    "\n",
    "        # run custom classification for min samples achieving min accuracy\n",
    "        if run_min_samples:\n",
    "\n",
    "            counter = 0\n",
    "            low_sample = LOW_SAMPLE_BOUND\n",
    "            high_sample = HIGH_SAMPLE_BOUND\n",
    "            current_sample = high_sample #(low_sample+high_sample)//2\n",
    "            list_samples = []\n",
    "        \n",
    "            while current_score > target_score and counter < 10:\n",
    "\n",
    "                # convert dataset to numpy arrays\n",
    "                X_classify = np.array(dataset,dtype=object)\n",
    "                y_classify = np.array(dataset_ind,dtype='int64')\n",
    "                \n",
    "                # choose malware sample subset for training\n",
    "                mw_classify_idx = np.random.permutation(len(X_classify))[:current_sample]\n",
    "                X_train = X_classify[mw_classify_idx]\n",
    "                y_train = y_classify[mw_classify_idx]\n",
    "\n",
    "                # truncate and pad input sequences\n",
    "                X_train = sequence.pad_sequences(X_train, maxlen=max_sequence_length)\n",
    "\n",
    "                # select classifier(s)\n",
    "                if select_classifier == 'svm':\n",
    "                    current_score, precision, recall = support_vector_machine(X_train, y_train, real_malware, target_names, max_sequence_length, classify_test_size)\n",
    "                elif select_classifier == 'rf': \n",
    "                    current_score, precision, recall = random_forest(X_train, y_train, real_malware, target_names, max_sequence_length, classify_test_size)\n",
    "                elif select_classifier == 'knn': \n",
    "                    current_score, precision, recall = k_nearest_neighbors(X_train, y_train, real_malware, target_names, max_sequence_length, classify_test_size)\n",
    "                elif select_classifier == 'lstm': \n",
    "                    current_score, precision, recall = lstm(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "                elif select_classifier == 'bi-lstm': \n",
    "                    current_score, precision, recall = bi_lstm(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "                elif select_classifier == 'cnn-lstm': \n",
    "                    current_score, precision, recall = cnn_lstm(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "                elif select_classifier == 'all': \n",
    "                    # all classifiers\n",
    "                    current_score = get_classification(dataset, dataset_ind, current_sample)\n",
    "                else:\n",
    "                    print('Error: Unknown Classifier')\n",
    "\n",
    "                # Debug\n",
    "                #print(counter, low_sample, current_sample, high_sample, current_score)\n",
    "\n",
    "                # keep track of samples that are above target_score\n",
    "                if current_score > target_score:\n",
    "                    list_samples.append(current_sample)\n",
    "                \n",
    "                if current_score > target_score:\n",
    "                    high_sample = current_sample # search upper samples\n",
    "                else:\n",
    "                    low_sample = current_sample # search lower samples\n",
    "\n",
    "                # End and prepare for next search iteration\n",
    "                counter += 1\n",
    "                current_sample = (low_sample+high_sample)//2\n",
    "\n",
    "                # Debug\n",
    "                #print('Current Score = ', str(current_score))\n",
    "                #print('Target Score = ', str(target_score))\n",
    "                #print('Current Counter = ', str(counter))\n",
    "                #print('Current Sample = ', str(current_sample))\n",
    "\n",
    "                # Stop conditions\n",
    "                if current_sample == HIGH_SAMPLE_BOUND - 1 or current_sample == LOW_SAMPLE_BOUND + 1:\n",
    "                    print('Reached final sample near bounds')\n",
    "                    break \n",
    "            \n",
    "            # report conclusion\n",
    "            if list_samples:\n",
    "                min_sample = min(list_samples)\n",
    "                print(\"Min Sample Size: \", min_sample)\n",
    "            else:\n",
    "                print(\"No samples can reach target score\")\n",
    "                print('Current Score = ', str(current_score))\n",
    "                min_sample = -1\n",
    "            \n",
    "            # save min samples\n",
    "            classifier_min_samples.append(min_sample)\n",
    "            classifier_min_sample_scores.append(current_score)\n",
    "        \n",
    "        # Run standard classification\n",
    "        else:\n",
    "            # select classifier(s)\n",
    "            if select_classifier == 'svm':\n",
    "                current_score, precision, recall = support_vector_machine(X_train, y_train, real_malware, target_names, max_sequence_length, classify_test_size)\n",
    "            elif select_classifier == 'rf': \n",
    "                current_score, precision, recall = random_forest(X_train, y_train, real_malware, target_names, max_sequence_length, classify_test_size)\n",
    "            elif select_classifier == 'knn': \n",
    "                current_score, precision, recall = k_nearest_neighbors(X_train, y_train, real_malware, target_names, max_sequence_length, classify_test_size)\n",
    "            elif select_classifier == 'lstm': \n",
    "                current_score, precision, recall = lstm(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "            elif select_classifier == 'bi-lstm': \n",
    "                current_score, precision, recall = bi_lstm(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "            elif select_classifier == 'cnn-lstm': \n",
    "                current_score, precision, recall = cnn_lstm(X_train, y_train, real_malware, target_names, max_sequence_length, embedding_vector_length, top_opcodes, classify_test_size)\n",
    "            elif select_classifier == 'all': \n",
    "                # all classifiers\n",
    "                current_score = get_classification(dataset, dataset_ind, mw_classify_size)\n",
    "            else:\n",
    "                print('Error: Unknown Classifier')\n",
    "    \n",
    "if run_min_samples:\n",
    "    print('===================================')\n",
    "    print('Classifiers: ', list_classifiers)\n",
    "    print('Min Samples for at least', str(target_score), 'Accuracy:', classifier_min_samples)\n",
    "    print('Min Sample Scores Accuracy:', classifier_min_sample_scores)\n",
    "    print('===================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
